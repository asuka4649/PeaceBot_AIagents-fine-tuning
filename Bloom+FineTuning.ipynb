!pip install transformers datasets accelerate
!pip install --upgrade transformers

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import os

# Load the tokenizer and model
model_name = "bigscience/bloom-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(
        [p + " " + c for p, c in zip(examples['prompt'], examples['completion'])],
        truncation=True,
        padding='max_length',
        max_length=256
    )

# Load dataset and split into train and validation sets (90% train, 10% validation)
dataset = load_dataset('json', data_files='/content/Manji_Training_Dataset.json', split='train[:90%]')
eval_dataset = load_dataset('json', data_files='/content/Manji_Training_Dataset.json', split='train[90%:]')

# Tokenize the datasets
tokenized_train_dataset = dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)


# Corrected Tokenization Function
def tokenize_function(examples):
    return tokenizer(
        [p + " " + c for p, c in zip(examples['prompt'], examples['completion'])],
        truncation=True,
        padding='max_length',
        max_length=256
    )

# Apply the corrected tokenization function
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Verify the tokenized data
print(tokenized_datasets)

from transformers import TrainingArguments, DataCollatorForLanguageModeling

# Correct Data Collator for causal language modeling
# data_collator = DataCollatorWithPadding(
#     tokenizer=tokenizer,
#     pad_to_multiple_of=8  # Optimizes memory usage
# )
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False # This is important for causal language modeling
)
# Correct Training Arguments
training_args = TrainingArguments(
    output_dir="./models/bloom-swastika-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=500,
    fp16=True,  # Use mixed precision if supported
    eval_strategy="no",  # âœ… Corrected parameter
    run_name="bloom-swastika-run-1"

)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    data_collator=data_collator,
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
trainer.save_model("./bloom-swastika-finetuned2")
tokenizer.save_pretrained("./bloom-swastika-finetuned2")


# Import necessary modules
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the fine-tuned model
model = AutoModelForCausalLM.from_pretrained("./bloom-swastika-finetuned2")
tokenizer = AutoTokenizer.from_pretrained("./bloom-swastika-finetuned2")

# Function to generate answers
def generate_answer(question, max_length=120, temperature=0.5):
    prompt = f"Question: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt")
    output_ids = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        temperature=temperature,
        top_k=50,
        do_sample=True,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, "").strip()

# Test the fine-tuned model
question = [
    "How can we address and solve the taboo around the swastika?"
]

print(f"Question: {question}\nAnswer: {generate_answer(question)}")


