!pip install langchain faiss-cpu openai anthropic streamlit cohere pandas requests
!pip install -U langchain-community
!pip install tiktoken
# PeaceBot Full Pipeline: RAG + Claude-2 + Source Classification + Optional RLHF Hook

# ============================
# STEP 1: Load Vector Store
# ============================

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
import anthropic
import os
import json
import requests

# Set API Keys
os.environ["OPENAI_API_KEY"] = "your_api"
os.environ["ANTHROPIC_API_KEY"] = "your_api"
os.environ["GOOGLE_API_KEY"] = "your_api"
os.environ["SEARCH_ENGINE_ID"] = "your_id"

# Initialize clients
client = anthropic.Client(api_key=os.environ["ANTHROPIC_API_KEY"])

# 1. Load your historian training data (JSON format)
with open("Manji_Training_Dataset.json", "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]  # Assumes JSONL format (one JSON object per line)

# 2. Convert to LangChain Document objects
documents = []
for item in data:
    prompt = item.get("prompt", "")
    completion = item.get("completion", "")
    content = f"{prompt.strip()} {completion.strip()}"
    documents.append(Document(page_content=content))

# 3. Initialize OpenAI embeddings
embeddings = OpenAIEmbeddings()  # Uses your OpenAI API key from env

# 4. Create the FAISS vector store
vectorstore = FAISS.from_documents(documents, embeddings)

# 5. Save the vector index locally (creates faiss_index/index.faiss and index.pkl)
vectorstore.save_local("faiss_index")

print("‚úÖ FAISS index created and saved at ./faiss_index/")

# ============================
# STEP 2: RAG Document Retrieval
# ============================

def retrieve_context(question, vectorstore, k=3):
    docs = vectorstore.similarity_search(question, k=k)
    return "\n".join([doc.page_content for doc in docs])

# ============================
# STEP 3: Prompt Construction
# ============================

def construct_prompt(context, few_shot, question):
    return f"""{anthropic.HUMAN_PROMPT}
You are a culturally sensitive historian chatbot helping to educate users about religious symbols.

Context:
{context}

Few-Shot Examples:
{few_shot}

Q: {question}
{anthropic.AI_PROMPT}"""

# ============================
# STEP 4: Claude-2 Generation
# ============================

# Claude AI Generation
def generate_claude_answer(question, few_shot_prompt):
    # Combine few-shot examples and user question into a single string
    prompt = few_shot_prompt + f"\n\nQ: {question}\nA:"

    # Send message to Claude 3 using Messages API
    response = client.messages.create(
        model="claude-3-opus-20240229",  # You can also use 'sonnet' or 'haiku'
        max_tokens=300,
        temperature=0.5,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    # Combine all content blocks into one string
    content_text = "".join(block.text for block in response.content)
    return content_text.strip()

# ============================
# STEP 5: AI Agent Source Classification
# ============================

def extract_few_shot_answers(prompt):
    answers = []
    examples = prompt.strip().split("Q: ")
    for ex in examples[1:]:
        if "\nA: " in ex:
            answers.append(ex.split("\nA: ")[1].strip())
    return answers

def fetch_web_results(query):
    url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={os.environ['GOOGLE_API_KEY']}&cx={os.environ['SEARCH_ENGINE_ID']}"
    res = requests.get(url)
    if res.status_code == 200:
        return [(item['title'], item['link']) for item in res.json().get("items", [])[:3]]
    return None

def is_likely_correct(text):
    heuristics = [
        "has been used for centuries", "commonly associated with", "scholars believe",
        "historically linked to", "widely regarded as", "some evidence suggests",
        "according to historical records", "symbol of good fortune"
    ]
    return any(h in text.lower() for h in heuristics)

def classify_answer(answer, few_shot):
    historian = extract_few_shot_answers(few_shot)
    for h in historian:
        if answer.lower().strip() in h.lower().strip():
            return "‚úÖ Historian Source", "Few-Shot Prompt", None
    web = fetch_web_results(answer)
    if web:
        return "üåê Web-Sourced", None, web
    if is_likely_correct(answer):
        return "‚ö†Ô∏è Likely Correct but Unverified", None, None
    return "‚ùå Likely Hallucination", None, None

# ============================
# MAIN EXECUTION BLOCK
# ============================

if __name__ == "__main__":
    question = "What is Manji? How Manji can achieve reconciliation with Hakenkreuz and its stigma?"

    # Load vector index and few-shot
    vectorstore = load_faiss_index()
    with open("few_shot_prompt.txt", "r", encoding="utf-8") as f:
     few_shot_examples = f.read()

    # RAG Context + Prompt
    context = retrieve_context(question, vectorstore)
    full_prompt = construct_prompt(context, few_shot_examples, question)

    # Claude Generation
    answer = generate_claude_answer(question, few_shot_examples)

    # AI Agent Classification
    classification, source, web_links = classify_answer(answer, few_shot_examples)

    # Print Results
    print("\nüîç Question:", question)
    print("\nüí¨ Claude Answer:", answer)
    print("\nüìä Classification:", classification)
    if source:
        print("Source:", source)
    if web_links:
        for title, link in web_links:
            print(f"Web: {title} - {link}")

  # Feedback Loop for RLHF
if classification in ["‚ö†Ô∏è Likely Correct but Unverified", "‚ùå Likely Hallucination"]:
    print("\nüß† Human Feedback Required!")
    print("Do you agree with the AI's answer? (yes / no / revise)")
    feedback = input("Your response: ").strip().lower()

    if feedback == "yes":
        feedback_label = "‚úÖ Marked as Correct by Human"
    elif feedback == "no":
        feedback_label = "‚ùå Rejected by Human"
    elif feedback == "revise":
        revised = input("Please provide your corrected answer:\n")
        feedback_label = "‚úèÔ∏è Human Revision Provided"
    else:
        feedback_label = "‚ö™Ô∏è No Action Taken"

    # Log the feedback for retraining/future use
    with open("rlhf_feedback_log.jsonl", "a", encoding="utf-8") as f:
        entry = {
            "question": question,
            "original_answer": answer,
            "classification": classification,
            "human_feedback": feedback_label,
            "revision": revised if feedback == "revise" else None
        }
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    print(f"\nüìù Feedback Logged: {feedback_label}")

import json

def convert_feedback_to_finetune_dataset(input_path="rlhf_feedback_log.jsonl", output_path="rlhf_training_dataset.jsonl"):
    converted_data = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line in f:
            entry = json.loads(line)
            if entry.get("human_feedback") == "revise" and entry.get("human_input"):
                converted_data.append({
                    "prompt": entry["question"],
                    "completion": entry["human_input"]
                })

    with open(output_path, "w", encoding="utf-8") as f_out:
        for item in converted_data:
            f_out.write(json.dumps(item) + "\n")

    print(f"‚úÖ Created training file with {len(converted_data)} examples at {output_path}")


from google.colab import drive
drive.mount('/content/drive')
!cp /content/rlhf_training_dataset.jsonl /content/drive/MyDrive/PeaceBot/
