# PeaceBot project (AI agents and fine-tuning)
This project is for an independent research project for "Peace Bot" project to create fine-tuned AI chatbot to generate balanced and less biased responses about Swastika, a sacred symbol of ancient Asian and Native American religion.

This research paper aims integrating social issue and AI engineering techniques resaerch to achieve religious and political reconciliation on machine. 
Peace Bot project has been submitted to HCI International 2025 (currently under reviewed).

# Extended Abstracts
This paper presents The Peace Bot Project, an interdisciplinary initiative that combines artificial
intelligence (AI), cultural reconciliation, and computational ethics to address stigmatized perceptions of
the swastika in Western-developed language models. Rooted in the teachings and scholarship of Dr. Rev.
TK Nakagaki—author of The Buddhist Swastika and Hitler’s Cross: Rescuing a Symbol of Peace from
Forces of Hate—the project investigates how cultural bias embedded in AI reflects a wider
misunderstanding of the swastika's ancient and peaceful significance in Eastern traditions.

Despite its millennia-long role as a sacred symbol of well-being in Buddhism, Hinduism, and Jainism, the
swastika has become stigmatized in the West due to its appropriation by the Nazi regime as the
Hakenkreuz. This semantic collapse has led to cultural tensions and prejudice, often directed at Eastern
religious communities. The project’s central hypothesis posits that large language models (LLMs), which
are predominantly trained on Western media and datasets, may reflect this historical bias by conflating the
swastika with hate-based ideologies, and thus fail to deliver accurate or contextually nuanced responses.

To investigate this, we evaluated eight popular LLMs—including GPT-4, Claude, Gemini, Cohere, and
BLOOM—using a set of 18 prompts across four categories: cultural context, historical comparison,
controversial discussions, and ethical reconciliation. Each response was rated using a multi-dimensional
scoring rubric assessing ethical framing, bias awareness, nuance, and cultural sensitivity. The findings
revealed that while some LLMs—particularly GPT-4 and Claude—provided thoughtful and nuanced
responses, others demonstrated clear gaps in accuracy and context. Notably, BLOOM exhibited
significant improvement after targeted fine-tuning.

To address these discrepancies, we implemented a twofold methodology. First, we applied fine-tuning
using JSON-based Q&A datasets (for BLOOM) and advanced prompt engineering (for Claude), including
few-shot learning, chain-of-thought prompting, and role-play scenarios. Post-training results showed
marked performance gains. Claude achieved a perfect average score of 5.0 across all categories, while
BLOOM doubled its average rating. These improvements underscore the potential for cultural retraining
of LLMs to become what we call “Peace Bots”—agents capable of delivering factually correct,
historically nuanced, and culturally empathetic information in response to complex or sensitive queries.

During this process, an additional challenge emerged: how to check whether the generated response is
based on legitimate online sources or merely generated by the model. This raises concerns about trust,
misinformation, and interpretability in LLM outputs, especially when such outputs involve historically
and emotionally charged symbols like the swastika.

To address this, we extended the project by designing and developing an AI agentic classifier system
capable of categorizing LLM responses into three categories:
1. Scientist-sourced (drawn from our curated model Q&A dataset);
2. Web-sourced (traceable to online public information);
3. Unverified: potentially hallucinated (plausible-sounding but untraceable responses).

Component: “check_response_origin()” function
This is the core agentic module. It:
● Receives the Claude-generated response.
● Independently checks if the response matches any known "few-shot" expert Q&A (i.e.
trusted/historian-sourced).
● If not found, it queries Google’s Custom Search API to find a potential web source.
● Finally, if no criteria are met, it classifies the response as “unverified”.

While the accuracy of the classification system is still under refinement, early testing has produced
promising results. The agent is capable of reliably retrieving relevant web sources and flagging responses
that lack verifiable references. In future iterations, we plan to implement a human-in-the-loop
reinforcement learning mechanism to further assess unverified outputs—distinguishing between plausible
but unsupported claims and clear hallucinations. This additional layer will support more precise
fine-tuning of the model's generative behavior. Ultimately, such an agentic framework could serve as a
monitoring and validation layer for generative AI systems deployed in sensitive domains such as civic
education, journalism, and public-facing conversational interfaces.

The Peace Bot Project contributes to emerging conversations in AI ethics, computational cultural studies,
and socially responsible design by demonstrating how LLMs can be trained to mitigate bias and recover
lost historical meaning. It also provides a real-world case study of how agentic AI architectures can be
layered on top of generative models to verify and explain outputs in high-stakes contexts.
Looking ahead, we aim to expand the dataset for broader religious and historical contexts, deploy the
Peace Bot in educational settings, and collaborate with AI ethics organizations and interfaith groups that
face similar challenges of symbolic misrepresentation. Additionally, a risk framework will be
implemented to anticipate public backlash or misuse, particularly among communities affected by the
trauma associated with World War II iconography. Ultimately, this research calls for more inclusive,
transparent, and historically grounded AI systems that support peace, education, and global cultural
understanding.

Keywords:
prompt engineering, fine-tuning, bias study, swastika, Hakenkreuz, reconciliation, machine learning, AI,
chat-bot, LLM
